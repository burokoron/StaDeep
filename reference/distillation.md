# 知識蒸留に関する論文・ライブラリ等まとめ

[2018年](#2018年)

## 2018年

- [Born Again Neural Networks](https://arxiv.org/abs/1805.04770) (3月)
  - 教師と生徒を同一サイズにして知識蒸留すると精度が上がる (CIFAR-100で+~0.2%)
  - ResNet⇒DenseNetのように構造を変更しても精度向上 (CIFAR-100で+~3%)
- 蒸留 (8月～10月)
  - [第1回](http://ai.deepx.co.jp/2018/08/28/%e8%92%b8%e7%95%99-%e7%ac%ac1%e5%9b%9e/)
    - 精度を落とさずにより小さなネットワークに圧縮する「蒸留」という手法のまとめ
    - 単にモデルを圧縮するだけでなく精度向上・複雑なタスクの効率化・敵対的攻撃の防御などにも利用できる
    - どういう課題にどういうアプローチをとれるかのまとめ(蒸留以外含む)
  - [第2回](http://ai.deepx.co.jp/2018/09/25/%e8%92%b8%e7%95%99-%e7%ac%ac2%e5%9b%9e/)
    - 分類では温度パラメータ付き Softmax 出力値を一致させるように学習する
    - 回帰では教師より大きく間違えた場合に強く学習するようにlossを調整する
    - 精度を落とさずに高速化ができる
  - [第3回](http://ai.deepx.co.jp/2018/10/15/%e8%92%b8%e7%95%99-%e7%ac%ac3%e5%9b%9e/)
    - 学習された複数モデルの結果を用いて半教師あり学習する手法の紹介
    - 同じモデル・同じデータで蒸留を繰り返す手法の紹介
    - モデル圧縮ではなく精度向上に貢献する手法のまとめ
