# 知識蒸留に関する論文・ライブラリ等まとめ

[2018年](#2018年)

## 2018年

- [Born Again Neural Networks](https://arxiv.org/abs/1805.04770) (3月)
  - 教師と生徒を同一サイズにして知識蒸留すると精度が上がる (CIFAR-100で+~0.2%)
  - ResNet⇒DenseNetのように構造を変更しても精度向上 (CIFAR-100で+~3%)
- 蒸留 (2018年8月～)
  - [第1回](http://ai.deepx.co.jp/2018/08/28/%e8%92%b8%e7%95%99-%e7%ac%ac1%e5%9b%9e/)
    - 精度を落とさずにより小さなネットワークに圧縮する「蒸留」という手法のまとめ
    - 単にモデルを圧縮するだけでなく精度向上・複雑なタスクの効率化・敵対的攻撃の防御などにも利用できる
    - どういう課題にどういうアプローチをとれるかのまとめ(蒸留以外含む)
  - [第2回](http://ai.deepx.co.jp/2018/09/25/%e8%92%b8%e7%95%99-%e7%ac%ac2%e5%9b%9e/)
    - 分類では温度パラメータ付き Softmax 出力値を一致させるように学習する
    - 回帰では教師より大きく間違えた場合に強く学習するようにlossを調整する
    - 精度を落とさずに高速化ができる
