# 知識蒸留に関する論文・ライブラリ等まとめ

[2018年](#2018年)

## 2018年

- [Born Again Neural Networks](https://arxiv.org/abs/1805.04770) (3月)
  - 教師と生徒を同一サイズにして知識蒸留すると精度が上がる (CIFAR-100で+~0.2%)
  - ResNet⇒DenseNetのように構造を変更しても精度向上 (CIFAR-100で+~3%)
  